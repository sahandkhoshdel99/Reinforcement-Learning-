{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NpXRTk3i7ko8"
   },
   "source": [
    "<br><br>\n",
    "\n",
    "<font color=#0C2C76 ><font>\n",
    "<p style = \"font-family:courier;\">\n",
    "    <font size=\"7\">\n",
    "        Interactive Learning \n",
    "    </font>    \n",
    "</p>\n",
    "\n",
    " <br>\n",
    "    \n",
    "<font color=#CF3A69><font>\n",
    "<p style = \"font-family:optima;\">\n",
    "    <font size=\"6\">\n",
    "        Homework #2 - Single State Reinforcement Learning\n",
    "    </font>    \n",
    "<font color=#000000> <font>\n",
    "</p>\n",
    "\n",
    "> $\\textit{Sahand Khoshdel - 810196607}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HpFZ3ljQ7kpA"
   },
   "source": [
    "<font color=#0C2C76 ><font>\n",
    "<p style = \"font-family:georgia;\">\n",
    "    <font size=\"6\">\n",
    "        Question 1 <br><br><u><font size=\"4\"> <font color=\"#A3678F\" >Thompson Sampling Method - 10 Armed Bandit </u><font></font>\n",
    "    </font>    \n",
    "</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 367
    },
    "executionInfo": {
     "elapsed": 3548,
     "status": "error",
     "timestamp": 1605528149583,
     "user": {
      "displayName": "Sahand Khoshdel",
      "photoUrl": "",
      "userId": "16066543092178160189"
     },
     "user_tz": -210
    },
    "id": "4SBUyD_I7kpB",
    "outputId": "17098cb9-2c07-4db6-9216-17b52977b907"
   },
   "outputs": [],
   "source": [
    "\n",
    "import numpy as np \n",
    "import amalearn\n",
    "\n",
    "from amalearn.reward import GaussianReward\n",
    "from amalearn.environment.multi_armed_bandit import MutliArmedBanditEnvironment \n",
    "from amalearn.agent import AgentBase"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "e724pZM_7kpG"
   },
   "source": [
    "<font color= #094CAF><font>\n",
    "<p style = \"font-family:cambria;\">\n",
    "    <font size=\"5\">\n",
    "       1.1: Bayesian Decision-Making Approach. \n",
    "    <font>\n",
    "</p>\n",
    "    \n",
    "<font color= #00000><font>\n",
    "\n",
    "<p style = \"font-family:cambria;\">\n",
    "    <font size=\"3\">\n",
    "        <b>Thompson sampling method</b> belongs to a <b>larger family</b> of Desicion Making approaches called <b>Bayesian-Based</b> approaches, Bayesian-Based approaches <b> aim to find the best action by updating a prior belief about actions distribution parameters.</b>  This is done by <b> conducting experimental trials</b> where the <b> agent first forms a prior belief $\\mathit{P\\,(\\theta\\,)\\,}$  about the reward distribution parameters of every action, $\\mathit{a_{i}}$ , then in each trial (t), picks the best action until now  </b> ( according to the outcome of previous trials ), <b>(  $\\mathit{A_{t} = a^{opt}_{t}}$ )</b> and <b> recieves the reward</b>  from the environment <b> ( $\\mathit{R_{t}}$ ) </b> . Next, the <b> agent updates the parameter distribution for the action taken at trial t (according to the bayes rule.) </b> In other words, it uses the <b>posterior of each trial as a prior for the next trial</b>.<br><br>        \n",
    "        <font>\n",
    "            </p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OCae5BEl7kpH"
   },
   "source": [
    "\n",
    "$\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\textit{Figure 1.1 - A Demonstration of how a Bayesian-Based Descision Making Approach works}$\n",
    " <br><br>\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6V0Jrnbb7kpI"
   },
   "source": [
    "<p style = \"font-family:cambria;\">\n",
    "    <font size=\"3\">\n",
    "      The level of <b> uncertainty for desicion making decays overtime </b>  as the <b> deviation of our estimation decreases</b>  representing a <b> more precise estimation </b> and smoothly moving from an exploration strategy to an exploitation one . Suggesting a Smart Approach for dealing with the <b> exploration-exploitation dilemma </b>, makes <b> Bayesian-Based Sampling approaches one of the most common used strategies in Desicion Making Problems</b>.  Sample Rewards drawn from their distribution play their role by forming the likelihood term . Therefore the reward received after performing the current trial ( $\\mathit{R_{t}}$ ) affects likelihood at trial t and prior at next trials.<br><br><br>In the next section we will have a closer look at the steps we should exactly follow to develop the bayesian framework and update our parameter estimation.<br><br>\n",
    "<font>\n",
    "    </p>\n",
    "       \n",
    "<font color= #094CAF><font>\n",
    "<p style = \"font-family:cambria;\">\n",
    "    <font size=\"4\">\n",
    "        Bayesian Framework in Desicion Making\n",
    "    <font>\n",
    "</p>        \n",
    "      \n",
    "        \n",
    "<font color= #00000><font>\n",
    "<p style = \"font-family:cambria;\">\n",
    "    <font size=\"3\">\n",
    "    This  which a <b> known initial distribution</b> ( <b> prior belief </b>$\\mathit{P\\,(\\theta\\,)\\,}$ ) is assumed over the <b> parameters</b>  of an outcome <b> (reward) </b> $\\mathit{(\\,q_{i}\\,)}$ (which can be represented as a <b> vector Q </b> which every <b> q value </b> corresponds to each action and <b> Q has n elements for a n-armed bandit problem</b>) <br><br> The basis of <b> bayesian-based methods</b> lie on <b> combining our prior belief about the parameters of the distribution </b>  with the <b> relative evidence (likelihood) we observe </b> ( how likely it is that the data we are observing belongs to a distribution with those parameters) and <b> producing a posterior distribution </b> for our parameters which finally illustrates  the <b> probability of these parameters being the true parameters </b>  of the data ( in our case 'reward' ) distribution.<br><br> What makes sampling based-approaches so important is that <b> this relation between prior and posterior distribution can act as an update our estimation</b> , each time we draw a sample an observe its outcome ( reward ) <br><br> As we <b> draw more and more samples</b> , our <b> parameter estimation becomes more and more accurate</b> , (This is beautifully demonstrated by the <b> Weak Law of Large Numbers</b>) <br><br><br><b>Important assunption </b>: <br><br> We are <b> assuming that the true action values are stationary,</b> in the case their not, the likelihood in each step is calculated with respect a varibale reference distribution and we will not necessarily converge to the reward distribution.<br><br><br>\n",
    "        <font>\n",
    "</p> \n",
    "    \n",
    "\n",
    "\n",
    "<font color= #094CAF><font>\n",
    "<p style = \"font-family:cambria;\">\n",
    "    <font size=\"4\">\n",
    "        Thompson Sampling Method:\n",
    "    <font>\n",
    "</p>   \n",
    "      \n",
    "<font color= #00000><font>\n",
    "<p style = \"font-family:cambria;\">\n",
    "    <font size=\"3\">\n",
    "        <b> Thompson Sampling Method </b> is a <b> bayesian decision making </b> method which is based on <b> choosing the action that maximizes the expected reward with respect to a randomly drawn belief.</b> The <b> belief is updated after drawing each sample with respect to the likelihood of the belief to the sample data </b> The <b> parameter from the Reward distribution </b>  we tend to estimate in thompson sampling method, is the <b> mean </b> \n",
    "        <font>\n",
    "</p> \n",
    "            \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WLmQLCT07kpI"
   },
   "source": [
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 3542,
     "status": "aborted",
     "timestamp": 1605528149581,
     "user": {
      "displayName": "Sahand Khoshdel",
      "photoUrl": "",
      "userId": "16066543092178160189"
     },
     "user_tz": -210
    },
    "id": "egyXhqD37kpJ"
   },
   "outputs": [],
   "source": [
    "EPISODE_LENGTH = ARM_NUMBERS = 10\n",
    "\n",
    "# Assigning mean and deviation for reward distribution:\n",
    "mean = [0]\n",
    "std = [1]\n",
    "\n",
    "# Create an instance 'reward' from class 'GaussianReward' with given parameters '(mean,std)', ... creating a list of ARM_NUMBER copies of 'reward':\n",
    "reward = GaussianReward(mean,std)\n",
    "rewards = [reward for i in range(ARM_NUMBERS)]\n",
    "\n",
    "enviroment1 = MutliArmedBanditEnvironment(rewards, EPISODE_LENGTH, 'env1')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hVdV1ItZ7kpN"
   },
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 3540,
     "status": "aborted",
     "timestamp": 1605528149582,
     "user": {
      "displayName": "Sahand Khoshdel",
      "photoUrl": "",
      "userId": "16066543092178160189"
     },
     "user_tz": -210
    },
    "id": "iTjbu8mB7kpN"
   },
   "outputs": [],
   "source": [
    "class ThompsonAgent(AgentBase):\n",
    "    def __init__(self,iteration,qvalue):\n",
    "        super(ThompsonAgent, self).__init__(rewards, episode_max_length, id, container)\n",
    "        self.iteration = \n",
    "        self.qvalue = "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bhmV_NIE7kpR"
   },
   "source": [
    "References : \n",
    "    1. https://web.stanford.edu/~bvr/pubs/TS_Tutorial.pdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 3538,
     "status": "aborted",
     "timestamp": 1605528149582,
     "user": {
      "displayName": "Sahand Khoshdel",
      "photoUrl": "",
      "userId": "16066543092178160189"
     },
     "user_tz": -210
    },
    "id": "Yn0LnTHU7kpR"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "name": "RL_asli_HW2_SahandKhoshdel_810196607.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
